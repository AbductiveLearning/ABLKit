{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Addition\n",
    "\n",
    "This notebook shows an implementation of [MNIST Addition](https://arxiv.org/abs/1805.10872). In this task, pairs of MNIST handwritten images and their sums are given, alongwith a domain knowledge base which contain information on how to perform addition operations. Our objective is to input a pair of handwritten images and accurately determine their sum.\n",
    "\n",
    "Intuitively, we first use a machine learning model (learning part) to convert the input images to digits (we call them pseudo labels), and then use the knowledge base (reasoning part) to calculate the sum of these digits. Since we do not have ground-truth of the digits, the reasoning part will leverage domain knowledge and revise the initial digits yielded by the learning part into results derived from abductive reasoning. This process enables us to further refine and retrain the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from examples.mnist_add.datasets import get_dataset\n",
    "from examples.models.nn import LeNet5\n",
    "from abl.learning import ABLModel, BasicNN\n",
    "from abl.reasoning import KBBase, Reasoner\n",
    "from abl.evaluation import ReasoningMetric, SymbolMetric\n",
    "from abl.utils import ABLLogger, print_log\n",
    "from abl.bridge import SimpleBridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Data\n",
    "\n",
    "First, we get the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_dataset(train=True, get_pseudo_label=True)\n",
    "test_data = get_dataset(train=False, get_pseudo_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets contain several data examples. In each data example, we have three components: X (a pair of images), gt_pseudo_label (a pair of corresponding ground truth digits, i.e., pseudo labels), and Y (their sum). The datasets are illustrated as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30000 data examples in the training set and 5000 data examples in the test set\n",
      "As an illustration, in the first data example of the training set, we have:\n",
      "X (2 images):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_pseudo_label (2 ground truth pseudo label): 7, 5\n",
      "Y (their sum result): 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(train_data[0])} data examples in the training set and {len(test_data[0])} data examples in the test set\")\n",
    "print(\"As an illustration, in the first data example of the training set, we have:\")\n",
    "print(f\"X ({len(train_data[0][0])} images):\")\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis('off') \n",
    "plt.imshow(train_data[0][0][0].numpy().transpose(1, 2, 0))\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis('off') \n",
    "plt.imshow(train_data[0][0][1].numpy().transpose(1, 2, 0))\n",
    "plt.show()\n",
    "print(f\"gt_pseudo_label ({len(train_data[1][0])} ground truth pseudo label): {train_data[1][0][0]}, {train_data[1][0][1]}\")\n",
    "print(f\"Y (their sum result): {train_data[2][0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Learning Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the learning part, we need to first build a base machine learning model. We use a simple [LeNet-5 neural network](https://en.wikipedia.org/wiki/LeNet) to complete this task, and encapsulate it within a `BasicNN` object to create the base model. `BasicNN` is a class that encapsulates a PyTorch model, transforming it into a base model with an sklearn-style interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = LeNet5(num_classes=10)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cls.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model = BasicNN(\n",
    "    cls,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    batch_size=32,\n",
    "    num_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BasicNN` offers methods like `predict` and `predict_prob`, which are used to predict the outcome class index and the probabilities for an image, respectively. As shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pred_idx for a batch of 32 samples: (32,)\n",
      "Shape of pred_prob for a batch of 32 samples: (32, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_idx = base_model.predict(X=[torch.randn(1, 28, 28).to(device) for _ in range(32)])\n",
    "print(f\"Shape of pred_idx for a batch of 32 examples: {pred_idx.shape}\")\n",
    "pred_prob = base_model.predict_proba(X=[torch.randn(1, 28, 28).to(device) for _ in range(32)])\n",
    "print(f\"Shape of pred_prob for a batch of 32 examples: {pred_prob.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, base model built above are trained to make predictions on instance-level data, i.e., a single image, and can not directly utilize example-level data, i.e., a pair of images. Therefore, we then wrap the base model into `ABLModel` which enables the learning part to train, test, and predict on example-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ABLModel(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 示例展示ablmodel和base model的predict的不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from abl.structures import ListData\n",
    "# data_examples = ListData()\n",
    "# data_examples.X = [list(torch.randn(2, 1, 28, 28)) for _ in range(3)]\n",
    "\n",
    "# model.predict(data_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Reasoning Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reasoning part, we first build a knowledge base which contain information on how to perform addition operations. We build it by creating a subclass of `KBBase`. In the derived subclass, we have to first initialize the `pseudo_label_list` parameter specifying list of possible pseudo labels, and then override the `logic_forward` function defining how to perform (deductive) reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddKB(KBBase):\n",
    "    def __init__(self, pseudo_label_list=list(range(10))):\n",
    "        super().__init__(pseudo_label_list)\n",
    "\n",
    "    # Implement the deduction function\n",
    "    def logic_forward(self, nums):\n",
    "        return sum(nums)\n",
    "\n",
    "kb = AddKB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knowledge base can perform logical reasoning. Below is an example of performing (deductive) reasoning: # TODO: ABDUCTIVE REASONING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning result of pseudo label sample [1, 2] is 3.\n"
     ]
    }
   ],
   "source": [
    "pseudo_label_example = [1, 2]\n",
    "reasoning_result = kb.logic_forward(pseudo_label_example)\n",
    "print(f\"Reasoning result of pseudo label example {pseudo_label_example} is {reasoning_result}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In addition to building a knowledge base based on `KBBase`, we can also establish a knowledge base with a ground KB using `GroundKB`, or a knowledge base implemented based on Prolog files using `PrologKB`. The corresponding code for these implementations can be found in the `main.py` file. Those interested are encouraged to examine it for further insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a reasoner by instantiating the class ``Reasoner``. Due to the indeterminism of abductive reasoning, there could be multiple candidates compatible to the knowledge base. When this happens, reasoner can minimize inconsistencies between the knowledge base and pseudo labels predicted by the learning part, and then return only one candidate which has highest consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoner = Reasoner(kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: During creating reasoner, the definition of \"consistency\" can be customized within the `dist_func` parameter. In the code above, we employ a consistency measurement based on confidence, which calculates the consistency between the data example and candidates based on the confidence derived from the predicted probability. In `main.py`, we provide options for utilizing other forms of consistency measurement.\n",
    "\n",
    "Also, during process of inconsistency minimization, one can leverage [ZOOpt library](https://github.com/polixir/ZOOpt) for acceleration. Options for this are also available in `main.py`. Those interested are encouraged to explore these features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up evaluation metrics. These metrics will be used to evaluate the model performance during training and testing. Specifically, we use `SymbolMetric` and `ReasoningMetric`, which are used to evaluate the accuracy of the machine learning model’s predictions and the accuracy of the final reasoning results, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = [SymbolMetric(prefix=\"mnist_add\"), ReasoningMetric(kb=kb, prefix=\"mnist_add\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge Learning and Reasoning\n",
    "\n",
    "Now, the last step is to bridge the learning and reasoning part. We proceed this step by creating an instance of `SimpleBridge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = SimpleBridge(model, reasoner, metric_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training and testing by invoking the `train` and `test` methods of `SimpleBridge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/19 14:41:46 - abl - INFO - Abductive Learning on the MNIST Addition example.\n",
      "12/19 14:41:46 - abl - INFO - loop(train) [1/5] segment(train) [1/3] \n",
      "12/19 14:41:51 - abl - INFO - model loss: 1.81279\n",
      "12/19 14:41:51 - abl - INFO - loop(train) [1/5] segment(train) [2/3] \n",
      "12/19 14:41:56 - abl - INFO - model loss: 1.40474\n",
      "12/19 14:41:56 - abl - INFO - loop(train) [1/5] segment(train) [3/3] \n",
      "12/19 14:42:01 - abl - INFO - model loss: 1.17817\n",
      "12/19 14:42:01 - abl - INFO - Evaluation start: loop(val) [1]\n",
      "12/19 14:42:02 - abl - INFO - Evaluation ended, mnist_add/character_accuracy: 0.496 mnist_add/reasoning_accuracy: 0.336 \n",
      "12/19 14:42:02 - abl - INFO - Saving model: loop(save) [1]\n",
      "12/19 14:42:02 - abl - INFO - Checkpoints will be saved to results/20231219_14_41_46/weights/model_checkpoint_loop_1.pth\n",
      "12/19 14:42:02 - abl - INFO - loop(train) [2/5] segment(train) [1/3] \n",
      "12/19 14:42:07 - abl - INFO - model loss: 0.85932\n",
      "12/19 14:42:07 - abl - INFO - loop(train) [2/5] segment(train) [2/3] \n",
      "12/19 14:42:11 - abl - INFO - model loss: 0.62120\n",
      "12/19 14:42:11 - abl - INFO - loop(train) [2/5] segment(train) [3/3] \n",
      "12/19 14:42:16 - abl - INFO - model loss: 0.35382\n",
      "12/19 14:42:16 - abl - INFO - Evaluation start: loop(val) [2]\n",
      "12/19 14:42:17 - abl - INFO - Evaluation ended, mnist_add/character_accuracy: 0.980 mnist_add/reasoning_accuracy: 0.961 \n",
      "12/19 14:42:17 - abl - INFO - Saving model: loop(save) [2]\n",
      "12/19 14:42:17 - abl - INFO - Checkpoints will be saved to results/20231219_14_41_46/weights/model_checkpoint_loop_2.pth\n",
      "12/19 14:42:17 - abl - INFO - loop(train) [3/5] segment(train) [1/3] \n",
      "12/19 14:42:21 - abl - INFO - model loss: 0.08302\n",
      "12/19 14:42:21 - abl - INFO - loop(train) [3/5] segment(train) [2/3] \n",
      "12/19 14:42:25 - abl - INFO - model loss: 0.05917\n",
      "12/19 14:42:25 - abl - INFO - loop(train) [3/5] segment(train) [3/3] \n",
      "12/19 14:42:30 - abl - INFO - model loss: 0.05425\n",
      "12/19 14:42:30 - abl - INFO - Evaluation start: loop(val) [3]\n",
      "12/19 14:42:31 - abl - INFO - Evaluation ended, mnist_add/character_accuracy: 0.988 mnist_add/reasoning_accuracy: 0.976 \n",
      "12/19 14:42:31 - abl - INFO - Saving model: loop(save) [3]\n",
      "12/19 14:42:31 - abl - INFO - Checkpoints will be saved to results/20231219_14_41_46/weights/model_checkpoint_loop_3.pth\n",
      "12/19 14:42:31 - abl - INFO - loop(train) [4/5] segment(train) [1/3] \n",
      "12/19 14:42:35 - abl - INFO - model loss: 0.04650\n",
      "12/19 14:42:35 - abl - INFO - loop(train) [4/5] segment(train) [2/3] \n",
      "12/19 14:42:39 - abl - INFO - model loss: 0.04175\n",
      "12/19 14:42:39 - abl - INFO - loop(train) [4/5] segment(train) [3/3] \n",
      "12/19 14:42:44 - abl - INFO - model loss: 0.04207\n",
      "12/19 14:42:44 - abl - INFO - Evaluation start: loop(val) [4]\n",
      "12/19 14:42:45 - abl - INFO - Evaluation ended, mnist_add/character_accuracy: 0.990 mnist_add/reasoning_accuracy: 0.979 \n",
      "12/19 14:42:45 - abl - INFO - Saving model: loop(save) [4]\n",
      "12/19 14:42:45 - abl - INFO - Checkpoints will be saved to results/20231219_14_41_46/weights/model_checkpoint_loop_4.pth\n",
      "12/19 14:42:45 - abl - INFO - loop(train) [5/5] segment(train) [1/3] \n",
      "12/19 14:42:49 - abl - INFO - model loss: 0.03484\n",
      "12/19 14:42:49 - abl - INFO - loop(train) [5/5] segment(train) [2/3] \n",
      "12/19 14:42:53 - abl - INFO - model loss: 0.03319\n",
      "12/19 14:42:53 - abl - INFO - loop(train) [5/5] segment(train) [3/3] \n",
      "12/19 14:42:58 - abl - INFO - model loss: 0.03510\n",
      "12/19 14:42:58 - abl - INFO - Evaluation start: loop(val) [5]\n",
      "12/19 14:42:59 - abl - INFO - Evaluation ended, mnist_add/character_accuracy: 0.993 mnist_add/reasoning_accuracy: 0.987 \n",
      "12/19 14:42:59 - abl - INFO - Saving model: loop(save) [5]\n",
      "12/19 14:42:59 - abl - INFO - Checkpoints will be saved to results/20231219_14_41_46/weights/model_checkpoint_loop_5.pth\n",
      "12/19 14:42:59 - abl - INFO - Evaluation ended, mnist_add/character_accuracy: 0.988 mnist_add/reasoning_accuracy: 0.976 \n"
     ]
    }
   ],
   "source": [
    "# Build logger\n",
    "print_log(\"Abductive Learning on the MNIST Addition example.\", logger=\"current\")\n",
    "log_dir = ABLLogger.get_current_instance().log_dir\n",
    "weights_dir = osp.join(log_dir, \"weights\")\n",
    "\n",
    "bridge.train(train_data, loops=5, segment_size=1/3, save_interval=1, save_dir=weights_dir)\n",
    "bridge.test(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c8d454494e49869a4ee4046edcac9a39ff683f7d38abf0769f648402670238e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
