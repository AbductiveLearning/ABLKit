{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from abl.abducer.abducer_base import AbducerBase\n",
    "from abl.abducer.kb import add_KB\n",
    "\n",
    "from abl.utils.plog import logger\n",
    "from abl.models.basic_model import BasicModel\n",
    "from abl.models.wabl_models import WABLBasicModel\n",
    "\n",
    "from models.nn import LeNet5\n",
    "from datasets.get_mnist_add import get_mnist_add\n",
    "from abl import framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:===========================================================\n",
      "INFO:root:============= Result Recorder Version: 0.03 ===============\n",
      "INFO:root:===========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize logger\n",
    "recorder = logger()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize knowledge base and abducer\n",
    "kb = add_KB(GKB_flag=True)\n",
    "abducer = AbducerBase(kb, dist_func=\"confidence\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize necessary component for machine learning part\n",
    "cls = LeNet5(num_classes=len(kb.pseudo_label_list))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cls.parameters(), lr=0.001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BasicModel\n",
    "# The function of BasicModel is to wrap NN models into the form of an sklearn estimator\n",
    "base_model = BasicModel(\n",
    "    cls,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    save_interval=1,\n",
    "    save_dir=recorder.save_dir,\n",
    "    batch_size=32,\n",
    "    num_epochs=1,\n",
    "    recorder=recorder,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use WABL model to join two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WABL model\n",
    "# The main function of the WABL model is to serialize data and \n",
    "# provide a unified interface for different machine learning models\n",
    "model = WABLBasicModel(base_model, kb.pseudo_label_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data\n",
    "train_X, train_Z, train_Y = get_mnist_add(train=True, get_pseudo_label=True)\n",
    "test_X, test_Z, test_Y = get_mnist_add(train=False, get_pseudo_label=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seg_idx:0, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 1.6403421089053154s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.45026259310543537s'}\n",
      "INFO:root:loop: 1 {'Character level accuracy': 0.099, 'ABL accuracy': 0.029}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 1.9688767925262451\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 1.9688767925262451\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.8824481889605522s'}\n",
      "INFO:root:seg_idx:1, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.3438392709940672s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.34831187315285206s'}\n",
      "INFO:root:loop: 2 {'Character level accuracy': 0.1754, 'ABL accuracy': 0.0798}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 1.5468237173080444\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 1.5468237173080444\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.7942761313170195s'}\n",
      "INFO:root:seg_idx:2, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.22833974659442902s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.33289023116230965s'}\n",
      "INFO:root:loop: 3 {'Character level accuracy': 0.2282, 'ABL accuracy': 0.1394}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 1.3011555437088014\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 1.3011555437088014\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.799525685608387s'}\n",
      "INFO:root:seg_idx:3, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.225564643740654s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.3387970831245184s'}\n",
      "INFO:root:loop: 4 {'Character level accuracy': 0.2849, 'ABL accuracy': 0.22}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 1.0837339675903321\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 1.0837339675903321\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.8926889058202505s'}\n",
      "INFO:root:seg_idx:4, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.22966895811259747s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.3328546490520239s'}\n",
      "INFO:root:loop: 5 {'Character level accuracy': 0.3779, 'ABL accuracy': 0.2826}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 0.8953762698173523\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 0.8953762698173523\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.7907911762595177s'}\n",
      "INFO:root:seg_idx:5, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.2257226835936308s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.33061812072992325s'}\n",
      "INFO:root:loop: 6 {'Character level accuracy': 0.4478, 'ABL accuracy': 0.333}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 0.7675108342170716\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 0.7675108342170716\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.7982648648321629s'}\n",
      "INFO:root:seg_idx:0, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.2379826307296753s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.30500760301947594s'}\n",
      "INFO:root:loop: 7 {'Character level accuracy': 0.6437, 'ABL accuracy': 0.4608}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 0.5691682313919068\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 0.5691682313919068\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.7903914619237185s'}\n",
      "INFO:root:seg_idx:1, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.2368618119508028s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.20389026775956154s'}\n",
      "INFO:root:loop: 8 {'Character level accuracy': 0.9198, 'ABL accuracy': 0.8458}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 0.18639301270544528\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 0.18639301270544528\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.7831189632415771s'}\n",
      "INFO:root:seg_idx:2, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.23463214747607708s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.17010959051549435s'}\n",
      "INFO:root:loop: 9 {'Character level accuracy': 0.9683, 'ABL accuracy': 0.9378}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 0.09258905411027372\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 0.09258905411027372\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.7852634787559509s'}\n",
      "INFO:root:seg_idx:3, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.23087852075695992s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.17247396148741245s'}\n",
      "INFO:root:loop: 10 {'Character level accuracy': 0.9714, 'ABL accuracy': 0.9442}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 0.09513531124591827\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 0.09513531124591827\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.789272129535675s'}\n",
      "INFO:root:seg_idx:4, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.23323991149663925s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.17015977017581463s'}\n",
      "INFO:root:loop: 11 {'Character level accuracy': 0.9758, 'ABL accuracy': 0.9522}\n",
      "INFO:root:model fitting\n",
      "INFO:root:0/1 model training loss is 0.07674169104583561\n",
      "INFO:root:Saving model and opter\n",
      "INFO:root:Model fitted, minimal loss is 0.07674169104583561\n",
      "INFO:root:#Result# {'func:': 'train: cost 0.7871940564364195s'}\n",
      "INFO:root:seg_idx:5, part num:6, data num:30000\n",
      "INFO:root:Start Predict Probability \n",
      "INFO:root:#Result# {'func:': 'predict: cost 0.2340849284082651s'}\n",
      "INFO:root:#Result# {'func:': 'batch_abduce: cost 0.16600692830979824s'}\n",
      "INFO:root:loop: 12 {'Character level accuracy': 0.9745, 'ABL accuracy': 0.9492}\n",
      "INFO:root:model fitting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m framework\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      3\u001b[0m     model,\n\u001b[1;32m      4\u001b[0m     abducer,\n\u001b[1;32m      5\u001b[0m     (train_X, train_Z, train_Y),\n\u001b[1;32m      6\u001b[0m     (test_X, test_Z, test_Y),\n\u001b[1;32m      7\u001b[0m     loop_num\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     sample_num\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39m# Save results\u001b[39;00m\n\u001b[1;32m     13\u001b[0m recorder\u001b[39m.\u001b[39mdump()\n",
      "File \u001b[0;32m~/ABL-Package/examples/mnist_add/../../abl/framework.py:86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, abducer, train_data, test_data, loop_num, sample_num, verbose)\u001b[0m\n\u001b[1;32m     83\u001b[0m finetune_X, finetune_Z \u001b[39m=\u001b[39m filter_data(X, abduced_Z)\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(finetune_X) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     85\u001b[0m     \u001b[39m# model.valid(finetune_X, finetune_Z)\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     train_func(finetune_X, finetune_Z)\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     INFO(\u001b[39m\"\u001b[39m\u001b[39mlack of data, all abduced failed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(finetune_X))\n",
      "File \u001b[0;32m~/ABL-Package/examples/mnist_add/../../abl/utils/plog.py:97\u001b[0m, in \u001b[0;36mResultRecorder.clock.<locals>.clocked\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclocked\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     96\u001b[0m     t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> 97\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    100\u001b[0m     name \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m~/ABL-Package/examples/mnist_add/../../abl/models/wabl_models.py:137\u001b[0m, in \u001b[0;36mWABLBasicModel.train\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    135\u001b[0m _data_Y, _ \u001b[39m=\u001b[39m merge_data(Y)\n\u001b[1;32m    136\u001b[0m data_Y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapping[y], _data_Y))\n\u001b[0;32m--> 137\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls_list[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mdata_X, y\u001b[39m=\u001b[39;49mdata_Y)\n",
      "File \u001b[0;32m~/ABL-Package/examples/mnist_add/../../abl/models/basic_model.py:301\u001b[0m, in \u001b[0;36mBasicModel.fit\u001b[0;34m(self, data_loader, X, y)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m data_loader \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     data_loader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_loader(X, y)\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(data_loader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_epochs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstop_loss)\n",
      "File \u001b[0;32m~/ABL-Package/examples/mnist_add/../../abl/models/basic_model.py:264\u001b[0m, in \u001b[0;36mBasicModel._fit\u001b[0;34m(self, data_loader, n_epoch, stop_loss)\u001b[0m\n\u001b[1;32m    262\u001b[0m min_loss \u001b[39m=\u001b[39m \u001b[39m1e10\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epoch):\n\u001b[0;32m--> 264\u001b[0m     loss_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(data_loader)\n\u001b[1;32m    265\u001b[0m     recorder\u001b[39m.\u001b[39mprint(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_epoch\u001b[39m}\u001b[39;00m\u001b[39m model training loss is \u001b[39m\u001b[39m{\u001b[39;00mloss_value\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m     \u001b[39mif\u001b[39;00m min_loss \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m loss_value \u001b[39m<\u001b[39m min_loss:\n",
      "File \u001b[0;32m~/ABL-Package/examples/mnist_add/../../abl/models/basic_model.py:331\u001b[0m, in \u001b[0;36mBasicModel.train_epoch\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    328\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, target)\n\u001b[1;32m    330\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 331\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    332\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    334\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ABL/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ABL/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "framework.train(\n",
    "    model,\n",
    "    abducer,\n",
    "    (train_X, train_Z, train_Y),\n",
    "    (test_X, test_Z, test_Y),\n",
    "    loop_num=15,\n",
    "    sample_num=5000,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Save results\n",
    "recorder.dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
