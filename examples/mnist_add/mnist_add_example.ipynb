{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Addition\n",
    "\n",
    "This example shows a simple implementation of MNIST Addition, which was first introduced in [Manhaeve et al., 2018](https://arxiv.org/abs/1805.10872). In this task, the inputs are pairs of MNIST handwritten images, and the outputs are their sums.\n",
    "\n",
    "In Abductive Learning, we hope to first use learning part to map the input images to their digits (we call it pseudo labels), and then use reasoning part to calculate the summation of these pseudo labels to get the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abl.bridge import SimpleBridge\n",
    "from abl.evaluation import ReasoningMetric, SymbolMetric\n",
    "from abl.learning import ABLModel, BasicNN\n",
    "from abl.reasoning import KBBase, Reasoner\n",
    "from abl.utils import ABLLogger, print_log\n",
    "from examples.mnist_add.datasets import get_mnist_add\n",
    "from examples.models.nn import LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "First, we get training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_mnist_add(train=True, get_pseudo_label=True)\n",
    "test_data = get_mnist_add(train=False, get_pseudo_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are illustrated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30000 data examples in the training set and 5000 data examples in the test set\n",
      "Each of the data example has 3 components: X, gt_pseudo_label, and Y.\n",
      "As an illustration, in the First data example of the training set, we have:\n",
      "X (2 images):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAD1CAYAAADNj/Z6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKUklEQVR4nO3df6zVdR3H8XPuDy4iCHcRUnKRQIFkOFHzV+Z0maVhM5dZyzY1JWCUbmk/tkwrXWXRQB0xayVUarOcthhmFJpL5GflYgTiryyV3wgIXrjnnv6x/kh4fy+de++58H48/n2d+z3ff+7hyXe7n1OuVqvVEgCQVkO9bwAAqC8xAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAguaauvvADDZf15H0AXfC7zgfqfQsHzWcH1F/RZ4cnAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBIrqneNwBAbg2DBoV7+Z1H99KdHNiLlw4L9/ahneHetCv+v/foezeGe2Xt+nCvlScDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHLOGehGjUMGh/tzd48M99XvnRfu7dWOcD971afDvVyuxvtDbwv3hko412zob9aFe2Xzlp69AaAunr9hYrj//do5vXQn9XP/J1rD/Sfjju3R9/dkAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEjOOQPdqLJjV7hfevxfw72zFJ8D0FxuDPelp9wb7g2lcvz+J8fv39OmTD033Ddc/PZwr2za1I13A3RV07Ft4d7x4kvh3m9HfP3jfj4t3C86b0V8gW6w8A+nhnv/zfHn67CV7eHe/Hj870OpFJ8zUytPBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJOfQoe7UWQnnXzx2Vrh//eN/7s67OeTc3fZYuH/oxCnh3vR7hw5BPTTP3xvu65aeGe6jv/hkTe+/tqaf7prRpSU9ev36HvnmyQAApCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnHMGetG4b8R/DTtx54yarj/0tA3hfuaw58O9s1oO9wdXnhLuyy+cFe6DG/qHO9A3bfh8fEbK0uNmh/v4Z6d15+3QAzwZAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJJzzkAvqmzbFu7Hfq1nvy/76cJXxN+o/e4JO8L9tQ/GPz+4ID2/s2VCuPdbsibcO+PLAwfQ2Noa7rd8bn64b620h/vYufFO/XkyAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACTnnAG6bM+sN8J9ZNMRNV1/0ZffF+4tu5fXdH1g/9bMHBPulxy5ONzHzrsx3N+1rGfPUKF2ngwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyTlngP+qnHdyuP903B0FV4jPGViwe3C4H7lmY7h3FLw7sH8v33hWuD9+/u0FVxgYrsfNXBfulYKrU3+eDABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJOWcgkYb+/cP9nnnxOQJDG+NzBHZX94b7HdMuD/fm51eGO7B/uy47PdxXXD873FvK8TkCReauejjcZ286J9yX3faecB/w4NKDvicOjicDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHLOGUhkz6+Hh3vROQJFzllxdbgPX+QcAfh/NI04Jtx/9r2ZBVdoCdevbpwY7le1LomvXo7ffeY7VoX7rG9uDfdH/zgm3Cubt8Q3QCFPBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDknDNwGNk446xwXzHhrnDvLLj+svb4j4lHTI3/Vrij4PrAAXTEvz1T1n8y3F9e1BbuI771ZLgvL50d7o1HDwv3zzzxVLhf3/pCuP/gugvDfdRN8TkIFPNkAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEjOOQOHkPKkCeG+4ivxOQKN5YL2q8YnDVz74xnh3vZq/LfKcCja+PD4cB82cFfhNRo/9nq4V7ZtC/eOVzeEe8P74/cfUXopfkGNqrv3hPuWysCCK+wI136vxWecUDtPBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDknDPQl5xxYjjf98DccO8stcTXLzhH4PhfTg/3cd//S8H7w+Hn6EE7w/2R8QsKr3HC9Ph3q+22vn1GR7m5X7ivvTU+A2XK4CfC/Ve7jgr3th+tDvdKuNIVngwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyTlnoBc1DBgQ7ttvjr/zfGBDwTkCBS5/7oJwH3vDqnDv3Le3pveHQ9G61SPiF4wvvsa3r7wn3L/UdGW4D3kmPsVj4D/eCPddI/uHe0dLOdxPnx5/NjxyTHwGyu7O+LPj1juuCPdh2/v2OQyHA08GACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOScM9CLXrnmpHBffuKdNV3/6b3xt3q/flF7uFedIwBvMX7O1nBfffGewmt85MiC/bNzDuaW3mJzJT6jZGhjwQ3UaFn7vnC/dvb14T78LucI1JsnAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByzhnoRo0njA33R2/8bsEV4u8c31eNzxGYfvN14T5k55KC9wf+V2XNM+E+efGMwmusv+CH4d5Yru3/ZT19jsD9O1vDff4l54f78DXOEejrPBkAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAknPOwEFoGDAg3J+9OT4noLUh3ouc+tTV4d423zkC0NvGXrWy8DWTHvpUuD992n3ddTv7dcUL54b7qgUnhPuoBzaEe2VdfBYDfZ8nAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAknPo0EF45ZqTwv1vZ99Z0/X/9EZzuI+aVnDwR03vDvSUEVe+HO7nT4oPFHvhw/3CvW1R/Nvf77cr4p+vPhnuPlsOf54MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMk5Z+A/TptY+JKFN9xe8IojwvWfHXvC/ZYZU8O9ZdPygvcH+qLK9tfCvXHxqnAfs7g77wbeypMBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDnnDLxp31Hx94WXSqXS0Mb4HIEiN/1rcri3LHSOAAC9z5MBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDnnDHSjL7xyRrhv/WjxWQYA0Ns8GQCA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSc87Am5oXrSx8zeRjTil4xb6CfWOX7wcAeosnAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHLlarVarfdNAAD148kAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJPdvx8SA9zSwVzUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_pseudo_label (2 ground truth pseudo label): 7, 5\n",
      "Y (their sum result): 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(train_data[0])} data examples in the training set and {len(test_data[0])} data examples in the test set\")\n",
    "print(f\"Each of the data example has {len(train_data)} components: X, gt_pseudo_label, and Y.\")\n",
    "print(\"As an illustration, in the First data example of the training set, we have:\")\n",
    "print(f\"X ({len(train_data[0][0])} images):\")\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis('off') \n",
    "plt.imshow(train_data[0][0][0].numpy().transpose(1, 2, 0))\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis('off') \n",
    "plt.imshow(train_data[0][0][1].numpy().transpose(1, 2, 0))\n",
    "plt.show()\n",
    "print(f\"gt_pseudo_label ({len(train_data[1][0])} ground truth pseudo label): {train_data[1][0][0]}, {train_data[1][0][1]}\")\n",
    "print(f\"Y (their sum result): {train_data[2][0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build the basic learning model. We use a simple [LeNet neural network](https://en.wikipedia.org/wiki/LeNet) to complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = LeNet5(num_classes=10)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cls.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model = BasicNN(\n",
    "    cls,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    batch_size=32,\n",
    "    num_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model can predict the outcome class index and the probabilities for an image, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pred_idx for a batch of 32 samples: (32,)\n",
      "Shape of pred_prob for a batch of 32 samples: (32, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_idx = base_model.predict(X=[torch.randn(1, 28, 28).to(device) for _ in range(32)])\n",
    "print(f\"Shape of pred_idx for a batch of 32 samples: {pred_idx.shape}\")\n",
    "pred_prob = base_model.predict_proba(X=[torch.randn(1, 28, 28).to(device) for _ in range(32)])\n",
    "print(f\"Shape of pred_prob for a batch of 32 samples: {pred_prob.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we build an instance of `ABLModel`. The main function of `ABLModel` is to serialize data and provide a unified interface for different base machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ABLModel(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the logic part, we first build a knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build knowledge base and reasoner\n",
    "class AddKB(KBBase):\n",
    "    def __init__(self, pseudo_label_list):\n",
    "        super().__init__(pseudo_label_list)\n",
    "\n",
    "    # Implement the deduction function\n",
    "    def logic_forward(self, nums):\n",
    "        return sum(nums)\n",
    "\n",
    "kb = AddKB(pseudo_label_list=list(range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knowledge base can perform logical reasoning. Below is an example of performing (deductive) reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning result of pseudo label sample [1, 2] is 3.\n"
     ]
    }
   ],
   "source": [
    "pseudo_label_sample = [1, 2]\n",
    "reasoning_result = kb.logic_forward(pseudo_label_sample)\n",
    "print(f\"Reasoning result of pseudo label sample {pseudo_label_sample} is {reasoning_result}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a reasoner. It can help minimize inconsistencies between the knowledge base and pseudo labels predicted by the learning part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoner = Reasoner(kb, dist_func=\"confidence\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up evaluation metrics. These metrics will be used to evaluate the model performance during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = [SymbolMetric(prefix=\"mnist_add\"), ReasoningMetric(kb=kb, prefix=\"mnist_add\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge Learning and Reasoning\n",
    "\n",
    "Now, the last step is to bridge the learning and reasoning part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = SimpleBridge(model, reasoner, metric_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build logger\n",
    "print_log(\"Abductive Learning on the MNIST Addition example.\", logger=\"current\")\n",
    "log_dir = ABLLogger.get_current_instance().log_dir\n",
    "weights_dir = osp.join(log_dir, \"weights\")\n",
    "\n",
    "bridge.train(train_data, loops=5, segment_size=1/3, save_interval=1, save_dir=weights_dir)\n",
    "bridge.test(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c8d454494e49869a4ee4046edcac9a39ff683f7d38abf0769f648402670238e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
