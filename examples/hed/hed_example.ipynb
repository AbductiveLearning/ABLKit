{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from abl.evaluation import SemanticsMetric, SymbolMetric\n",
    "from abl.learning import ABLModel, BasicNN\n",
    "from abl.reasoning import PrologKB, ReasonerBase\n",
    "from abl.utils import ABLLogger, print_log, reform_list\n",
    "from examples.hed.datasets.get_hed import get_hed, split_equation\n",
    "from examples.hed.hed_bridge import HEDBridge\n",
    "from examples.models.nn import SymbolNet\n",
    "from zoopt import Dimension, Objective, Parameter, Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build logger\n",
    "print_log(\"Abductive Learning on the HED example.\", logger=\"current\")\n",
    "\n",
    "# Retrieve the directory of the Log file and define the directory for saving the model weights.\n",
    "log_dir = ABLLogger.get_current_instance().log_dir\n",
    "weights_dir = osp.join(log_dir, \"weights\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize knowledge base and abducer\n",
    "class HedKB(PrologKB):\n",
    "    def __init__(self, pseudo_label_list, pl_file):\n",
    "        super().__init__(pseudo_label_list, pl_file)\n",
    "\n",
    "    def consist_rule(self, exs, rules):\n",
    "        rules = str(rules).replace(\"'\", \"\")\n",
    "        return len(list(self.prolog.query(\"eval_inst_feature(%s, %s).\" % (exs, rules)))) != 0\n",
    "\n",
    "    def abduce_rules(self, pred_res):\n",
    "        prolog_result = list(self.prolog.query(\"consistent_inst_feature(%s, X).\" % pred_res))\n",
    "        if len(prolog_result) == 0:\n",
    "            return None\n",
    "        prolog_rules = prolog_result[0][\"X\"]\n",
    "        rules = [rule.value for rule in prolog_rules]\n",
    "        return rules\n",
    "    \n",
    "class HedReasoner(ReasonerBase):\n",
    "    def revise_at_idx(self, data_sample):\n",
    "        revision_idx = np.where(np.array(data_sample.flatten(\"revision_flag\")) != 0)[0]\n",
    "        candidate = self.kb.revise_at_idx(\n",
    "            data_sample.pred_pseudo_label, data_sample.Y, revision_idx\n",
    "        )\n",
    "        return candidate\n",
    "\n",
    "    def zoopt_revision_score(self, symbol_num, data_sample, sol):\n",
    "        revision_flag = reform_list(list(sol.get_x().astype(np.int32)), data_sample.pred_pseudo_label)\n",
    "        data_sample.revision_flag = revision_flag\n",
    "\n",
    "        lefted_idxs = [i for i in range(len(data_sample.pred_idx))]\n",
    "        candidate_size = []\n",
    "        while lefted_idxs:\n",
    "            idxs = []\n",
    "            idxs.append(lefted_idxs.pop(0))\n",
    "            max_candidate_idxs = []\n",
    "            found = False\n",
    "            for idx in range(-1, len(data_sample.pred_idx)):\n",
    "                if (not idx in idxs) and (idx >= 0):\n",
    "                    idxs.append(idx)\n",
    "                candidate = self.revise_at_idx(data_sample[idxs])\n",
    "                if len(candidate) == 0:\n",
    "                    if len(idxs) > 1:\n",
    "                        idxs.pop()\n",
    "                else:\n",
    "                    if len(idxs) > len(max_candidate_idxs):\n",
    "                        found = True\n",
    "                        max_candidate_idxs = idxs.copy()\n",
    "            removed = [i for i in lefted_idxs if i in max_candidate_idxs]\n",
    "            if found:\n",
    "                candidate_size.append(len(removed) + 1)\n",
    "                lefted_idxs = [i for i in lefted_idxs if i not in max_candidate_idxs]\n",
    "        candidate_size.sort()\n",
    "        score = 0\n",
    "        import math\n",
    "\n",
    "        for i in range(0, len(candidate_size)):\n",
    "            score -= math.exp(-i) * candidate_size[i]\n",
    "        return score\n",
    "    \n",
    "    def abduce(self, data_sample):\n",
    "        symbol_num = data_sample.elements_num(\"pred_pseudo_label\")\n",
    "        max_revision_num = self._get_max_revision_num(self.max_revision, symbol_num)\n",
    "\n",
    "        solution = self.zoopt_get_solution(symbol_num, data_sample, max_revision_num)\n",
    "\n",
    "        data_sample.revision_flag = reform_list(\n",
    "            solution.astype(np.int32), data_sample.pred_pseudo_label\n",
    "        )\n",
    "\n",
    "        abduced_pseudo_label = []\n",
    "\n",
    "        for single_instance in data_sample:\n",
    "            single_instance.pred_pseudo_label = [single_instance.pred_pseudo_label]\n",
    "            candidates = self.revise_at_idx(single_instance)\n",
    "            if len(candidates) == 0:\n",
    "                abduced_pseudo_label.append([])\n",
    "            else:\n",
    "                abduced_pseudo_label.append(candidates[0][0])\n",
    "        data_sample.abduced_pseudo_label = abduced_pseudo_label\n",
    "        return abduced_pseudo_label\n",
    "\n",
    "    def abduce_rules(self, pred_res):\n",
    "        return self.kb.abduce_rules(pred_res)\n",
    "\n",
    "kb = HedKB(\n",
    "    pseudo_label_list=[1, 0, \"+\", \"=\"], pl_file=\"./datasets/learn_add.pl\"\n",
    ")\n",
    "reasoner = HedReasoner(kb, dist_func=\"hamming\", use_zoopt=True, max_revision=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build necessary components for BasicNN\n",
    "cls = SymbolNet(num_classes=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(cls.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "# optimizer = torch.optim.Adam(cls.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BasicNN\n",
    "# The function of BasicNN is to wrap NN models into the form of an sklearn estimator\n",
    "base_model = BasicNN(\n",
    "    cls,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    batch_size=32,\n",
    "    num_epochs=1,\n",
    "    save_interval=1,\n",
    "    save_dir=weights_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ABLModel\n",
    "# The main function of the ABL model is to serialize data and\n",
    "# provide a unified interface for different machine learning models\n",
    "model = ABLModel(base_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up metrics\n",
    "metric_list = [SymbolMetric(prefix=\"hed\"), SemanticsMetric(prefix=\"hed\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge Machine Learning and Logic Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = HEDBridge(model, reasoner, metric_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_data = get_hed(train=True)\n",
    "train_data, val_data = split_equation(total_train_data, 3, 1)\n",
    "test_data = get_hed(train=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge.pretrain(\"./weights\")\n",
    "bridge.train(train_data, val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb6f4ceeabb9a733f366948eb80109f83aedf798cc984df1e68fb411adb27d58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
